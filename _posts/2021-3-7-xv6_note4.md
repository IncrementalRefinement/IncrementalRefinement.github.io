---
layout: post
title: XV6 note (4) --- Process
tags: [OS, XV6, 6.S081]
---

xv-6有关进程的代码主要在proc.c & proc.h这两个文件中

## Source code

```c
enum procstate { UNUSED, SLEEPING, RUNNABLE, RUNNING, ZOMBIE };

struct proc {
  struct spinlock lock;

  // p->lock must be held when using these:
  enum procstate state;        // Process state
  struct proc *parent;         // Parent process
  void *chan;                  // If non-zero, sleeping on chan
  int killed;                  // If non-zero, have been killed
  int xstate;                  // Exit status to be returned to parent's wait
  int pid;                     // Process ID

  // these are private to the process, so p->lock need not be held.
  uint64 kstack;               // Virtual address of kernel stack
  uint64 sz;                   // Size of process memory (bytes)
  pagetable_t pagetable;       // User page table
  struct trapframe *trapframe; // data page for trampoline.S
  struct context context;      // swtch() here to run process
  struct file *ofile[NOFILE];  // Open files
  struct inode *cwd;           // Current directory
  char name[16];               // Process name (debugging)
};
```

```c
struct proc proc[NPROC];
```

从上面代码我们可以看到，xv-6将进程的状态分为UNUSED, SLEEPING, RUNNABLE, RUNNING, ZOMBIE这五个，很容易理解，在此不赘述

在proc.c文件中有一个struct proc的数组，数组的大小在param.h中定义，xv-6中线程与进程(原版的xv-6源码只有进程没有线程这一概念，但是6.s081有实现线程的实验)都共享NPROC个slot，所以这也决定了内核支持的最大线程数量，如果NPROC定义的过大，可能会占用过多的内存，导致浪费

在struct proc结构中的成员在源码的注释中写的很清楚，我在此就不多废话了，让我们进入下一段代码吧

```c
static struct proc*
allocproc(void)
{
  struct proc *p;

  for(p = proc; p < &proc[NPROC]; p++) {
    acquire(&p->lock);
    if(p->state == UNUSED) {
      goto found;
    } else {
      release(&p->lock);
    }
  }
  return 0;

found:
  p->pid = allocpid();

  // Allocate a trapframe page.
  if((p->trapframe = (struct trapframe *)kalloc()) == 0){
    release(&p->lock);
    return 0;
  }

  // An empty user page table.
  p->pagetable = proc_pagetable(p);
  if(p->pagetable == 0){
    freeproc(p);
    release(&p->lock);
    return 0;
  }

  // Set up new context to start executing at forkret,
  // which returns to user space.
  memset(&p->context, 0, sizeof(p->context));
  p->context.ra = (uint64)forkret;
  p->context.sp = p->kstack + PGSIZE;

  return p;
}
```

这一部分代码的目的也很明确:遍历proc数组，找到一个UNUSED的proc结构，为这个结构中的pagetable、trapframe、contex分配内存，最后返回这个结构的指针。而freeproc()函数也可由这个函数反向类推。

```c
int
fork(void)
{
  int i, pid;
  struct proc *np;
  struct proc *p = myproc();

  // Allocate process.
  if((np = allocproc()) == 0){
    return -1;
  }

  // Copy user memory from parent to child.
  if(uvmcopy(p->pagetable, np->pagetable, p->sz) < 0){
    freeproc(np);
    release(&np->lock);
    return -1;
  }
  np->sz = p->sz;

  np->parent = p;

  // copy saved user registers.
  *(np->trapframe) = *(p->trapframe);

  // Cause fork to return 0 in the child.
  np->trapframe->a0 = 0;

  // increment reference counts on open file descriptors.
  for(i = 0; i < NOFILE; i++)
    if(p->ofile[i])
      np->ofile[i] = filedup(p->ofile[i]);
  np->cwd = idup(p->cwd);

  safestrcpy(np->name, p->name, sizeof(p->name));

  pid = np->pid;

  np->state = RUNNABLE;

  release(&np->lock);

  return pid;
}
```

我一直觉得fork这个操作是类Unix系统进程操作的精髓所在，我们现在就看到了fork在xv-6上初始的实现，首先alloc一个struct proc，然后设子进程的父进程PID，父进程的所有相关参数复制进子进程的pcb中，包括文件描述符，trapframe等。6.s081lab中有copy on write这一更加高效的方法，我也会在本文中写一下我对这个实验的理解。

```c
void
scheduler(void)
{
  struct proc *p;
  struct cpu *c = mycpu();
  
  c->proc = 0;
  for(;;){
    // Avoid deadlock by ensuring that devices can interrupt.
    intr_on();
    
    int nproc = 0;
    for(p = proc; p < &proc[NPROC]; p++) {
      acquire(&p->lock);
      if(p->state != UNUSED) {
        nproc++;
      }
      if(p->state == RUNNABLE) {
        // Switch to chosen process.  It is the process's job
        // to release its lock and then reacquire it
        // before jumping back to us.
        p->state = RUNNING;
        c->proc = p;
        swtch(&c->context, &p->context);

        // Process is done running for now.
        // It should have changed its p->state before coming back.
        c->proc = 0;
      }
      release(&p->lock);
    }
    if(nproc <= 2) {   // only init and sh exist
      intr_on();
      asm volatile("wfi");
    }
  }
}
```

这一部分代码是xv-6的进程调度代码，在main.c中可以看到：每个cpu在完成内核的初始化操作后就调用scheduler这一函数，而scheduler内部有一个无限循环，不断调配现存的线程，从这个角度看似乎内核的行为就像一个非常复杂以及完备的有限状态机

我们也可以看到这个调度算法是非常粗陋的，遍历一遍pcb数组，将所有RUNNABLE的进程一一执行，不能说是一个比较聪明的调度方法

## Summary

从上面可以看到，xv-6进程这部分的源码比较简单，时不时可以看到为了找到一个PID的进程遍历一遍整个数组的操作，甚至也可以说是显得有点低效，但毕竟是教学用魔改的内核，也不应苛责。

## My Q&A

### 1. 每一个进程如何初始化trapfram以及trampoline呢？

```c
extern char trampoline[]
```

文件头部这一段声明将trampoline声明为外部的一段内存，并在proc_pagetable()函数中mappages(pagetable, TRAMPOLINE, PGSIZE,(uint64)trampoline, PTE_R | PTE_X)

如何初始化trapframe这一部分没看到欸,,,好像proc.c中只有kalloc trapframe的内存，我好菜..... 之后试试能不能补上(已经在note2里面补上了，菜逼的redemption)

## 2. kernel_stack 怎么理解？

每一个进程(线程)都有自己的userstack和kerlnelstack，当从用户态进入内核态或相反情况时，会自动将stack切换到相应的stack。在用户态不能访问内核态的内存，反之也是一样(除非explicitly being requested by the kernel code， through functions like copy_from_user())。

这样的主要目的就是为了安全考虑，内核不能相信用户的栈是有效以及可靠的，如果没有kernel stack，用户完全可能fuck up自己的栈and do malicious thing intentionally or by accident

所以可以认为每个进程切换到内核态的时候栈指针自动指向了这个进程的kernel stack，而kernel没有一个stack of kernel，而是借用这些进程的kernel stack，这也符合在kernel再次出现trap该如何处理的情况。也注意：这些kernelstack在内核的va中没有一对一被直接映射而是映射到不同位置

## 3. copy on write & lazy allocation 实验的思路?

这几个实验鸽了好久，时间比较紧，所以对着源码和实验要求写一下自己的思路，等这波面试差不多了我就搞，在做了在做了,,,, /(ㄒoㄒ)/~~
