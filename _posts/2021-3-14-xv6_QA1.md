---
layout: post
title: XV6 Q&A (1)
tags: [OS, XV6, 6.S081]
---

列出老大难&下午看书冒出来的几个问题，单独开个篇幅解决一下

1. **为什么trap handler中会选用自旋锁，自旋锁在哪些场景下停用广泛？解释一下下面这段话里的overkill**

    Semaphores cause tasks to sleep on contention, which is unacceptable for interrupt handlers. Basically, for such a short and fast task (interrupt handling) the work carried out by the semaphore is overkill. Also, spinlocks can't be held by more than one task.

    可以参看 [这个回答](https://stackoverflow.com/questions/5869825/when-should-one-use-a-spinlock-instead-of-mutex)，我摘出了一些个人觉得比较有意思的句子如下

    >If now the mutex was only locked for a very short amount of time, the time spent in putting a thread to sleep and waking it up again might exceed the time the thread has actually slept by far and it might even exceed the time the thread would have wasted by constantly polling on a spinlock

    >Using spinlocks on a single-core/single-CPU system makes usually no sense, since as long as the spinlock polling is blocking the only available CPU core, no other thread can run and since no other thread can run, the lock won't be unlocked either. IOW, a spinlock wastes only CPU time on those systems for no real benefit. If the thread was put to sleep instead, another thread could have ran at once, possibly unlocking the lock and then allowing the first thread to continue processing, once it woke up again.

    >On a multi-core/multi-CPU systems, with plenty of locks that are held for a very short amount of time only, the time wasted for constantly putting threads to sleep and waking them up again might decrease runtime performance noticeably. When using spinlocks instead, threads get the chance to take advantage of their full runtime quantum (always only blocking for a very short time period, but then immediately continue their work), leading to much higher processing throughput.

    >A hybrid spinlock behaves like a normal spinlock at first, but to avoid wasting too much CPU time, it may have a back-off strategy. It will usually not put the thread to sleep (since you don't want that to happen when using a spinlock), but it may decide to stop the thread (either immediately or after a certain amount of time) and allow another thread to run, thus increasing chances that the spinlock is unlocked (a pure thread switch is usually less expensive than one that involves putting a thread to sleep and waking it up again later on, though not by far).

    如上，并不是说mutex一定是优于spinlock的选择，因为获取mutex失败而导致将当前线程sleep掉，并在mutex释放后等待shced再次运行当前线程运行的开销完全可能大于spinlockbusy wait一段时间带来的开销。

    当然，在单核的情况下spinlock的busywaiting完全是一种无意义的行为，唯一的CPU不可能在busy waiting的同时释放spinlock，这时用mutex将当前线程挂起才是明智的选择。而在多核的情况下则大不相同，spinlock完全可能被其他的CPU获取并在释放之后被wait获取。

    既然我们不知道程序会被跑在单核/多核的环境下，以及OS也不知道一段代码是否做了单核/多核的优化.事实上很多情况下要用到lock时采用的是一种hybrid spinlock。hybrid spinlock的介绍已经在上文中摘出，不再赘述。
2. **承接第一个问题，如果选用sleeplock会怎么样？以及为什么sleeplock不需要关闭inter**

    已经在note5中回答
3. **怎么保证出现一个由于换页导致的缺页，把页换回之后，页不会在线程再次得到执行之前被换出？或者说可不可能出现这种反复横跳的情况，造成巨大开销？(这个问题可能过于宏大,看着答吧)**
4. **解释一下内核线程和用户线程一对多，一对一，多对多，xv6中是一对一吗？如何理解(书93页)**
5. **产生page fault的时候，traphandler是如何处理的？他是如何区分是违规访问，invalid，COW write等不同情况以及xv-6将page fault分为哪些情况，有哪些玩法？**
6. **承接上一条，如果尝试违规写只读，这种情况与cow中的写只读怎么区分？**

    5 & 6，这个问题的解决方法其实非常简单，内核区分pagefault的实现大致如下:
    1. r_stval返回的是产生pagefault时尝试访问的内存位置，只要判断该内存位置的合法性即可(比如>当前分配的sz，>MAXVA,访问guardpage，访问页表中不存在的页等)此时直接将用户态进程终止，exit(-1)即可。
    2. 在xv6中，区分lazy allocation和cow可以通过PTE的RSW保留位来判断的，在cow的情况下主动设置&清零RSW位即可，我认为可以这样实现。

7. **一直说的COW实验的思路呢？就在这里补上吧**

    首先给出MIT官方的提示，然后我解释一下自己的看法，狗尾续貂，现在还没有完成这个实验，完全是处于结合源码和自己的猪脑空想，先拟定一个方针
    >Here's a reasonable plan of attack.
    >
    >1. Modify uvmcopy() to map the parent's physical pages into the child, instead of allocating new pages. Clear PTE_W in the PTEs of both child and parent.
    >1. Modify usertrap() to recognize page faults. When a page-fault occurs on a COW page, allocate a new page with kalloc(), copy the old page to the new page, and install the new page in the PTE with PTE_W set.
    >2. Ensure that each physical page is freed when the last PTE reference to it goes away -- but not before. A good way to do this is to keep, for each physical page, a "reference count" of the number of user page tables that refer to that page. Set a page's reference count to one when kalloc() allocates it. Increment a page's reference count when fork causes a child to share the page, and decrement a page's count each time any process drops the page from its page table. kfree() should only place a page back on the free list if its reference count is zero. It's OK to to keep these counts in a fixed-size array of integers. You'll have to work out a scheme for how to index the array and how to choose its size. For example, you could index the array with the page's physical address divided by 4096, and give the array a number of elements equal to highest physical address of any page placed on the free list by kinit() in kalloc.c.
    >3. Modify copyout() to use the same scheme as page faults when it encounters a COW page.

    >Some hints:
    >
    >1. The lazy page allocation lab has likely made you familiar with much of the xv6 kernel code that's relevant for copy-on-write. However, you should not base this lab on your lazy allocation solution; instead, please start with a fresh copy of xv6 as directed above.
    >2. It may be useful to have a way to record, for each PTE, whether it is a COW mapping. You can use the RSW (reserved for software) bits in the RISC-V PTE for this.
    >3. usertests explores scenarios that cowtest does not test, so don't forget to check that all tests pass for both.
    >4. Some helpful macros and definitions for page table flags are at the end of kernel/riscv.h.
    >5. If a COW page fault occurs and there's no free memory, the process should be killed.

    我总结一下自认为需要注意的点：
    1. uvmcopy()原本既需要复制pagetable友需要复制物理内存，但在COW的策略下只需要修改pagetable中的entry就行了，同时将新旧pagetable里所有entry中的PTE_W清掉，标为不可写。但个人感觉这里其实还可以优化啊，比如说内存中的代码区原本就是只读的，这种情况COW也要区分啊，想要去修改代码段显然是不允许的，怎么能避免呢？
    2. 之后需要修改usertrap函数来认出COW导致的pagefault，如果出现这种情况再分配实际的物理内存，这里我觉得还是可以优化吧，比如fork之后接一个exec，需要分配的物理内存的量是非常大的，每一页都要用traphandler额外开销很麻烦，能够做到批处理吗？
    3. hints里面认为我们可以用pagetabele entry的RSW(reserved for software)位来分辨ROW
    4. 在没有实现COW之前，每当一个页被回收的时候，需要做的是更新pagetable以及将物理页放回freelist中区。但由于COW，很有可能多个虚拟页只读地映射到同一个物理页，这时候就需要额外的信息来记录当前的物理页被映射了多少次(有点像垃圾回收那种感觉)，当没有虚拟页指向物理页之后才将物理页回收，hints中推荐额外开一个数组来记录，直接用物理页的起始地址/4096后作为index。
    5. 以上内容纯属我个人的扯淡，我还没有get hands dirty来实现这些内容，实现之后我会再补充自己实操时遇到的问题

    虽然cow的usertest的第一个case就挂了，但还是补充一下自己实操这个实验时候遇到的问题&注意点吧:
    1. 父进程fork出多个子进程，cow时父进程页表以及所有子进程页表的可写项的RSW都被设置，并标为不可写，而当最后一个共享当前物理页的进程也产生了一个cow pagefault时，只需要改写该进程页表项即可，不需要重新kalloc & kfree(我认为可以做的一点优化，但估计也并不显著、而且自己写的这一部分还挂了....)
    2. copyout在内核态进行，此myproc()->pagetable的页表为内核进程的页表，此时也不能依赖usertrap来处理pagefault(应为此时的中断处理是由内核中断处理函数进行的)
    3. cow之后不需要也不应该mappage(此时会产生remap的异常，干扰正常的错误处理)，此时直接改写pte即可

        ```c
        *pte = PA2PTE(new_pa) | new_flag;
        ```

    4. cow只需要将可写的的pte标出RSWbits & 取消掉可写的位, 只读的位可以暂时不管(如果要改写，会在exec中处理)
    5. 不会用gdb，不会基本的调试方法，举步维艰，x86保护模式&汇编也不熟，我是菜逼我是菜逼我是菜逼。之后多线程的调试估计会更加麻烦吧，或许先看源码留下几篇笔记指路，等之后更熟练了再回了重新补上剩下的实验？
